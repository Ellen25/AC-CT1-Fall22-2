<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>week 7 hw</title>

   
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>

    <div class="mainpage">
        <a href="https://youtu.be/rdn5r5Q7ehY"> system 01: 19/60s</a>
        <div style="padding:42.71% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/774596841?h=30921bf90c&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;" title="输入/文字19/1"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>
        <!-- <img src="images/01.png"> -->
        <p>[click on the title to see the video]<br>controlling the led with a button, I turned the led on everytime I found my eye blinking. I tried to imitate the way that algorithm and machine learning see the world as accurate as possible, but it was not an easy case</p>
        <hr>

        <a href="https://youtube.com/shorts/QEeMi4ACf3w?feature=share">system 02: C</a>
        <img src="images/C.png">
        <p>
            a led blinking system representing C (C for Chenlan)
        </p>
        <hr>

        <a href="https://youtube.com/shorts/KpThdFD_WUE?feature=share">system 03: YCL</a>
        <img src="images/ycl.png">
        <p>
            a led blinking system representing the abbreviation of my name
        </p>
        <hr>

    </div>
    <!-- <div class="instructionDiv">
      <p class="inst">
      Instruction: keep pressing "a", "b", "c", and "d" according to the gesture instructed in the text. Press: space for classification; "S" to save the gestures; "L" to load and enjoy the generator!
      </p>
  </div>

    <div class="docs">
      <div class="examples">
        <img src="images/shot1.png">
        <img src="images/shot2.png">
        <img src="images/shot3.png">
        <img src="images/shot4.png">
      </div>

      <div class="documentation">
        <p>
          <b>Documentation & Reflection</b>
          <br>
          I think it would be interesting if the filter and stickers can be added in real-time. For instance, the stickers can be added when having synchronizing meetings through Zoom according to some gestures that the users are doing.
          I drew the stickers and used both KNN and PoseNet model so that different user can storage their own data and generate their stickers according to their gestures.
          However, sometimes it is difficult for the machine learning model to recognize similar gestures (e.g. hello and ok, which both have hand showing beside one's face).
          Therefore, this made me think that my experiment with machine learning model is still on the level of offering machine an input and it will try to provide an output that 
          fits my expectation, instead of make it generate its own creation based on the input and training materials.
        </p>
      </div>
    </div> -->
  </body>
</html>
